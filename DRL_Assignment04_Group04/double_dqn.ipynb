{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "from tqdm import tqdm \n",
    "import argparse\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # Disables TensorFlow logging messages\n",
    "# Disable TensorFlow debug info\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
    "# tf.debugging.experimental.disable_dump_debug_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Experience Replay Buffer ----------------------------\n",
    "\n",
    "class ExperienceReplayBuffer():\n",
    "    def __init__(self,max_size:int, env_name:str, parallel_game_unrolls:int,unroll_steps:int, obv_preprocessing_function:callable):\n",
    "        self.max_size = max_size\n",
    "        self.env_name = env_name\n",
    "        self.parallel_game_unrolls = parallel_game_unrolls\n",
    "        self.unroll_steps = unroll_steps\n",
    "        self.obv_preprocessing_function = obv_preprocessing_function\n",
    "\n",
    "        self.envs = gym.vector.make(env_name, num_envs=parallel_game_unrolls)\n",
    "        self.num_possible_actions = self.envs.single_action_space.n\n",
    "        self.current_states, _ = self.envs.reset() \n",
    "\n",
    "        self.data = []\n",
    "\n",
    "    def fill_with_samples(self,dqn_network,epsilon:float):\n",
    "        \n",
    "        states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "        subsequent_states_list = []\n",
    "        terminal_list = []\n",
    "\n",
    "        for i in range(self.unroll_steps):\n",
    "            actions = self.sample_epsilon_greedy(dqn_network,epsilon)\n",
    "            # print(actions)\n",
    "            next_observations, rewards, terminateds, _, _ = self.envs.step(actions) # in vectorized envs\n",
    "            # put the sample into the buffer ( s,a,r,s',t )\n",
    "            states_list.append(self.current_states)\n",
    "            actions_list.append(actions)\n",
    "            rewards_list.append(rewards)\n",
    "            subsequent_states_list.append(next_observations)\n",
    "            terminal_list.append(terminateds)\n",
    "            \n",
    "            # update the current state\n",
    "            self.current_states = next_observations\n",
    "\n",
    "        def data_generator():\n",
    "            for s_batch, a_batch, r_batch, s_prime_batch, t_batch in zip(states_list, actions_list, rewards_list, subsequent_states_list, terminal_list):\n",
    "                for game_idx in range(self.parallel_game_unrolls):\n",
    "                    s = s_batch[game_idx, :, :, :]\n",
    "                    a = a_batch[game_idx]\n",
    "                    r = r_batch[game_idx]\n",
    "                    s_prime = s_prime_batch[game_idx, :, :, :]\n",
    "                    t = t_batch[game_idx]\n",
    "                    # self.buffer.append((s, a, r, s_prime, t))\n",
    "                    yield (s, a, r, s_prime, t)\n",
    "\n",
    "        dataset_tensor_specs = (tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8, name='s'),\n",
    "                                tf.TensorSpec(shape=(), dtype=tf.int32, name='a'),\n",
    "                                tf.TensorSpec(shape=(), dtype=tf.float32, name='r'),\n",
    "                                tf.TensorSpec(shape=(210, 160, 3), dtype=tf.uint8, name='s_prime'),\n",
    "                                tf.TensorSpec(shape=(), dtype=tf.bool, name='t'))\n",
    "        \n",
    "        sample_dataset_tf = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
    "\n",
    "        # not optimal but showcase the steps clearly (it's applied 2-3 times )\n",
    "        sample_dataset_tf = sample_dataset_tf.map(lambda s, a, r, s_prime, t: \n",
    "                        (self.obv_preprocessing_function(s), a, r, self.obv_preprocessing_function(s_prime), t))\n",
    "        sample_dataset_tf = sample_dataset_tf.cache().shuffle(buffer_size=self.unroll_steps*self.parallel_game_unrolls,\n",
    "                                                              reshuffle_each_iteration=True)\n",
    "\n",
    "        # make sure that cache is applied on all the datapoints\n",
    "        for datapoint in sample_dataset_tf:\n",
    "            continue\n",
    "\n",
    "        self.data.append(sample_dataset_tf)\n",
    "        datapoints_in_buffer = len(self.data) * self.unroll_steps * self.parallel_game_unrolls\n",
    "        if datapoints_in_buffer > self.max_size:\n",
    "            self.data.pop(0)\n",
    "\n",
    "    def create_dataset_tf(self):\n",
    "        erp_dataset = tf.data.Dataset.sample_from_datasets(self.data,weights= [1/float(len(self.data)) for _ in self.data],\n",
    "                                                            stop_on_empty_dataset=False)\n",
    "        return erp_dataset\n",
    "\n",
    "    def sample_epsilon_greedy(self,dqn, epsilon:float):\n",
    "        observations = self.obv_preprocessing_function(self.current_states)\n",
    "\n",
    "        q_values = dqn(observations) # tensor of type tf.float32 shape (parallel_game_unrolls, num_actions)\n",
    "        gready_actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64 shape (parallel_game_unrolls,)\n",
    "        random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0,\n",
    "                                            maxval=self.num_possible_actions, dtype=tf.int64) # tensor of type tf.int64 shape (parallel_game_unrolls,)\n",
    "        epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,), minval=0, maxval=1, dtype=tf.float32) > epsilon # tensor of type tf.bool shape (parallel_game_unrolls,)\n",
    "        actions = tf.where(epsilon_sampling, gready_actions, random_actions).numpy() # tensor of type tf.int64 shape (parallel_game_unrolls,)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- network ----------------------------\n",
    "\n",
    "INPUT_RESHAPED_SIZE = (84, 84, 3)\n",
    "\n",
    "def obv_preprocessing_function(obsevation):\n",
    "    # obsevation = tf.image.resize(obsevation, size=INPUT_RESHAPED_SIZE[:2])\n",
    "    obsevation = tf.image.resize(obsevation, size=(84, 84))\n",
    "    obsevation = tf.cast(obsevation, tf.float32) / 128 -1.0\n",
    "    return obsevation\n",
    "\n",
    "def create_dqn_model(num_possible_actions:int):\n",
    "    # create the input layer\n",
    "    # input_layer = tf.keras.Input(shape=INPUT_RESHAPED_SIZE ,dtype=tf.float32 ,name=\"input_layer\")\n",
    "    input_layer = tf.keras.Input(shape=(84, 84, 3) ,dtype=tf.float32 ,name=\"input_layer\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', name='h_1')(input_layer)\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', name='h_2')(x) + x # residual connection\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', name='h_3')(x) + x \n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, name='pool_layer_1')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', name='h_4')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', name='h_5')(x) + x # residual connection\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', name='h_6')(x) + x\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, name='pool_layer_2')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_7')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_8')(x) + x # residual connection\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_9')(x) + x\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, name='pool_layer_3')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_10')(x) + x # residual connection\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_11')(x) + x \n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', name='h_12')(x) + x\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='pool_layer_4_global')(x)\n",
    "    x = tf.keras.layers.Dense(units=64, activation='relu', name='h_13_dense')(x) + x # residual connection\n",
    "    x = tf.keras.layers.Dense(units=num_possible_actions, activation='linear', name='output_layer')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "def train_dqn(dqn_network:tf.keras.Model,dqn_target_network:tf.keras.Model,\n",
    "              dataset,optimizer,gamma,max_training_steps:int,batch_size:int=128):\n",
    "    dataset = dataset.batch(batch_size).prefetch(4)\n",
    "\n",
    "    @tf.function\n",
    "    def training_step(q_target, obvs, actions):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_predictions_all_actions = dqn_network(obvs) # tensor of type tf.float32 shape (batch_size \"parallel_game_unrolls\", num_actions)\n",
    "            q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1) # axis=1 ? tensor of type tf.float32 shape (batch_size \"parallel_game_unrolls\",)\n",
    "            loss = tf.reduce_mean(tf.square(q_predictions - q_target))\n",
    "        gradients = tape.gradient(loss,dqn_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,dqn_network.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    losses = []\n",
    "    q_values = []\n",
    "    for i, state_transition in enumerate(dataset):\n",
    "        # train on data\n",
    "        state, action, reward, subsequent_state, terminated = state_transition\n",
    "        # calculate q_target\n",
    "        q_vals_target = dqn_target_network(subsequent_state)\n",
    "        q_values.append(q_vals_target.numpy())\n",
    "\n",
    "        q_vals_dqn = dqn_network(subsequent_state) # double dqn\n",
    "        max_q_value_idx = tf.argmax(q_vals_dqn, axis=1) # double dqn\n",
    "        # max_q_value = tf.reduce_max(q_vals_target, axis=1)\n",
    "        max_q_value = tf.gather(q_vals_target, max_q_value_idx, batch_dims=1) # double dqn\n",
    "\n",
    "        use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_value,dtype=tf.float32), tf.ones_like(max_q_value,dtype=tf.float32))\n",
    "\n",
    "        q_target = reward + ( gamma * max_q_value * use_subsequent_state) # if in terminal state, q_target = reward\n",
    "        loss = training_step(q_target,obvs=state, actions=action)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        if i > max_training_steps:\n",
    "            break\n",
    "\n",
    "    return np.mean(losses) , np.mean(q_values)\n",
    "\n",
    "def test_q_network(dqn_network, env_name:str, num_parallel_tests:int, max_steps_per_game:int, gamma:float,test_epsilon:float = 0.05):\n",
    "    envs = gym.vector.make(env_name, num_envs=num_parallel_tests) #  asynchronous=True ?\n",
    "    num_possible_actions = envs.single_action_space.n\n",
    "    states, _ = envs.reset()\n",
    "    \n",
    "    time_step = 0\n",
    "    done = False\n",
    "    # eposides_finished is to keep track of how many games have finished\n",
    "    episodes_finished = np.zeros(num_parallel_tests, dtype=bool)\n",
    "    returns = np.zeros(num_parallel_tests)\n",
    "\n",
    "    test_steps = 0\n",
    "    while not done and time_step < max_steps_per_game:\n",
    "        states = obv_preprocessing_function(states)\n",
    "        q_values = dqn_network(states)\n",
    "        greedy_actions = tf.argmax(q_values, axis=1)\n",
    "        random_actions = tf.random.uniform(shape=(num_parallel_tests,), minval=0,\n",
    "                                           maxval=num_possible_actions, dtype=tf.int64)\n",
    "        epsilon_sampling = tf.random.uniform(shape=(num_parallel_tests,), minval=0, maxval=1, dtype=tf.float32) > test_epsilon\n",
    "        actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy()\n",
    "\n",
    "        states, rewards, terminateds, _, _ = envs.step(actions)\n",
    "        \n",
    "        episodes_finished = np.logical_or(episodes_finished,terminateds)\n",
    "        # update returns only for games that are not finished yet\n",
    "        returns += ( (gamma**time_step) * rewards ) * np.logical_not(episodes_finished).astype(np.float32)\n",
    "\n",
    "        time_step += 1\n",
    "        done = np.all(episodes_finished)\n",
    "    \n",
    "        if test_steps % 100 == 0:\n",
    "            print(\"====================================\")\n",
    "            print(f\"test_steps: {test_steps}, returns: {returns} {np.sum(episodes_finished) / num_parallel_tests} , {terminateds.shape} , {episodes_finished.shape}\")\n",
    "        test_steps += 1\n",
    "\n",
    "    return np.mean(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- ddqn_algorithm -----------------\n",
    "\n",
    "def visualise_results(results:pd.DataFrame , step = None):\n",
    "    columns = results.columns.values\n",
    "    columns_count = len(columns)\n",
    "    fig, axes = plt.subplots(3,1,sharex=True,figsize=(10,5*columns_count))\n",
    "    fig.suptitle(f'DQN results - {step} steps')\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    for idx ,key in enumerate(columns):\n",
    "        sns.lineplot(data=results, x=results.index, y=key, ax=axes[idx])\n",
    "        axes[idx].grid()\n",
    "\n",
    "    timestring = datetime.datetime.now().strftime(\"%m_%d-%H_%M_%S\")\n",
    "    plt.savefig(f\"./results/dqn_results_{timestring}_{step}.png\")\n",
    "\n",
    "def polyak_averaging_weights(source_network:tf.keras.Model, target_network:tf.keras.Model, polyak_averaging_factor:float = 0.01):\n",
    "    source_network_weights = source_network.get_weights()\n",
    "    target_network_weights = target_network.get_weights()\n",
    "    averaged_weights = []\n",
    "    for sourc_weight, target_weight in zip(source_network_weights, target_network_weights):\n",
    "        fraction_of_kept_weights = polyak_averaging_factor * target_weight\n",
    "        fraction_of_updated_weights = (1 - polyak_averaging_factor) * sourc_weight\n",
    "        average_weight = fraction_of_kept_weights + fraction_of_updated_weights\n",
    "        averaged_weights.append(average_weight)\n",
    "    \n",
    "    target_network.set_weights(averaged_weights)\n",
    "\n",
    "\n",
    "def ddqn_algorithm(max_train_steps:int = 10000, max_test_steps:int = 1000, prefill_erp_steps:int = 100, save_testing_visuals:bool = False):\n",
    "    ENVIRONMENT_NAME = \"ALE/Breakout-v5\"\n",
    "    NUM_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
    "    ERP_MAX_SIZE = 100000\n",
    "    PARALLEL_GAME_UNROLLS = 16\n",
    "    UNROLL_STEPS = 4\n",
    "    OBV_PREPROCESSING_FUNC = obv_preprocessing_function\n",
    "\n",
    "    EPSILON = 0.2\n",
    "    GAMMA = 0.98\n",
    "    NUM_TRAINING_STEPS_PER_ITER = 4\n",
    "    NUM_TRAINING_ITERATION = max_train_steps\n",
    "    BATCH_SIZE = 16\n",
    "    PROGRESS_REPORT_INTERVAL = int(NUM_TRAINING_ITERATION * 0.2)\n",
    "    if PROGRESS_REPORT_INTERVAL == 0:\n",
    "        PROGRESS_REPORT_INTERVAL = 1\n",
    "    NUM_PARALLEL_TEST_ENVS = 16\n",
    "    MAX_STEPS_PER_GAME = max_test_steps\n",
    "    PREFILL_ERP_STEPS = prefill_erp_steps \n",
    "    POLYAK_AVERAGING_FACTOR = 0.99\n",
    "\n",
    "    erp = ExperienceReplayBuffer(max_size=ERP_MAX_SIZE,env_name=ENVIRONMENT_NAME,\n",
    "                                 parallel_game_unrolls=PARALLEL_GAME_UNROLLS,\n",
    "                                 unroll_steps=UNROLL_STEPS,\n",
    "                                 obv_preprocessing_function=OBV_PREPROCESSING_FUNC)\n",
    "    \n",
    "    # the network to be trained\n",
    "    dqn_agent = create_dqn_model(NUM_ACTIONS)\n",
    "    # the target network to calculate the q_targets (to address the moving target problem)\n",
    "    dqn_target = create_dqn_model(NUM_ACTIONS)\n",
    "    dqn_agent.summary()\n",
    "    dqn_optimizer = tf.keras.optimizers.Adam()\n",
    "    # test the network\n",
    "    dqn_agent(tf.random.uniform(shape=(1,84,84,3), minval=0, maxval=255, dtype=tf.float32))\n",
    "    dqn_target(tf.random.uniform(shape=(1,84,84,3), minval=0, maxval=255, dtype=tf.float32))\n",
    "    print(\"test_done\")\n",
    "    # copy the weights from the agent to the target\n",
    "    polyak_averaging_weights(dqn_agent, dqn_target, polyak_averaging_factor=POLYAK_AVERAGING_FACTOR)\n",
    "\n",
    "    return_tracker = []\n",
    "    dgn_prediction_erroe_tracker = []\n",
    "    average_q_values_tracker = []\n",
    "    test_iteration_tracker = []\n",
    "\n",
    "    # prefill the replay buffer\n",
    "    prefill_expploration = 1.0\n",
    "    for i in range(PREFILL_ERP_STEPS):\n",
    "        erp.fill_with_samples(dqn_agent, prefill_expploration) # epsilon = 1 -> always take random actions for the prefilling\n",
    "\n",
    "    iteration = 0\n",
    "    done = False\n",
    "\n",
    "    prog_bar = tqdm(total=NUM_TRAINING_ITERATION, desc=\"Training DDQN\")\n",
    "    while not done and iteration < NUM_TRAINING_ITERATION:\n",
    "        iteration += 1\n",
    "        # step 1: interact with environment and put some s,a,r,s' into replay buffer\n",
    "        erp.fill_with_samples(dqn_agent,EPSILON)\n",
    "        dataset = erp.create_dataset_tf()\n",
    "        # step 2: train the network on some samples from the replay buffer\n",
    "        mean_loss, mean_q_values = train_dqn(dqn_agent,dqn_target,dataset,dqn_optimizer,GAMMA,NUM_TRAINING_STEPS_PER_ITER,BATCH_SIZE)\n",
    "        # update the target network weights\n",
    "        polyak_averaging_weights(dqn_agent, dqn_target, polyak_averaging_factor=0.01)\n",
    "        prog_bar.update(1)\n",
    "        # step 3: test the network\n",
    "        if (iteration - 1) % PROGRESS_REPORT_INTERVAL == 0:\n",
    "            average_return = test_q_network(dqn_agent,ENVIRONMENT_NAME,NUM_PARALLEL_TEST_ENVS,\n",
    "                                            MAX_STEPS_PER_GAME,gamma=GAMMA)\n",
    "            return_tracker.append(average_return)\n",
    "            dgn_prediction_erroe_tracker.append(mean_loss)\n",
    "            average_q_values_tracker.append(mean_q_values)\n",
    "            test_iteration_tracker.append(iteration)\n",
    "            print(\"====================================\")\n",
    "            print(f\"######### TESTING , iteration: {iteration} :\")\n",
    "            print(\"average return: \", average_return)\n",
    "            print(\"mean loss: \", mean_loss)\n",
    "            print(\"mean q values estimates: \", mean_q_values)\n",
    "            print(\"====================================\")\n",
    "            results_df = pd.DataFrame({\"test_iteration\":test_iteration_tracker,\"average_return\":return_tracker,\n",
    "                                       \"average_q_values\":average_q_values_tracker, \"average_loss\":dgn_prediction_erroe_tracker})\n",
    "            results_df = results_df.set_index(\"test_iteration\")\n",
    "            timestring = datetime.datetime.now().strftime(\"%m_%d-%H_%M_%S\")\n",
    "            results_df.to_csv(f\"./results/dqn_results_{timestring}_{iteration}.csv\")\n",
    "            if save_testing_visuals:\n",
    "                visualise_results(results_df,iteration)\n",
    "        # done = True # some condition to stop the training\n",
    "    prog_bar.close()\n",
    "    \n",
    "    # make a dataframe to store the results\n",
    "    results_df = pd.DataFrame({\"test_iteration\":test_iteration_tracker,\"average_return\":return_tracker,\n",
    "                               \"average_q_values\":average_q_values_tracker,\"average_loss\":dgn_prediction_erroe_tracker})\n",
    "    results_df = results_df.set_index(\"test_iteration\")\n",
    "    timestring = datetime.datetime.now().strftime(\"%m_%d-%H_%M_%S\")\n",
    "    visualise_results(results_df,iteration)\n",
    "    results_df.to_csv(f\"./results/dqn_results_{timestring}_{iteration}.csv\")\n",
    "    dqn_agent.save_weights(f\"./checkpoints/dqn_agent_weights_{timestring}_{iteration}\")\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- main -----------------\n",
    "results = ddqn_algorithm(max_train_steps=1000,\n",
    "                        max_test_steps=1000,\n",
    "                        prefill_erp_steps=100,\n",
    "                        save_testing_visuals=True)\n",
    "\n",
    "visualise_results(results, step='final')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
